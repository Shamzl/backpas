## Repository Structure

### `dataset/` – Datasets used in the pipeline

Each subfolder inside `dataset/` corresponds to a specific dataset.
The folder should be named after the dataset itself.
For example, `dataset/MIS/` corresponds to the MIS dataset.

Inside each dataset folder, you will find the following subdirectories:

* **`instance/`**
  Contains all the original instances that compose the dataset.
  All instances should be placed here, regardless of their purpose (training, validation, or testing).
  It is recommended to use prefixes like `train_`, `valid_`, or `test_` to maintain organization.

* **`backbone/`**
  Stores the extracted backbones for the instances located in the `instance/` folder.
  This directory is generated by running the script:

  ```
  src/0_run_backbone_extraction.sh
  ```

* **`backbone_extraction_log/`**
  Contains log files and metadata about the backbone extraction process for each instance.
  While not strictly necessary, this folder is useful for recording extraction times and diagnostic information.
  It is automatically created when running:

  ```
  src/0_run_backbone_extraction.sh
  ```

* **`ml_dataset_variables/`** and **`ml_dataset_literals/`**
  Contain the machine learning datasets derived from the instances and their backbones.

  * `ml_dataset_variables/` includes bipartite graphs where **variable nodes** are connected to **constraint nodes**.
  * `ml_dataset_literals/` includes bipartite graphs where **literal nodes** are connected to **constraint nodes**.
    These folders are generated by running:

  ```
  src/1_create_ml_dataset.py
  ```

---

### `src/` – Source code for the pipeline

This directory contains all the main scripts that compose the backbone extraction and prediction pipeline.

#### Execution Scripts

* **`0_run_backbone_extraction.sh`**
  Bash helper script that runs the backbone extractor (Guroback) on a folder of instances.

* **`1_create_ml_dataset.py`**
  Generates the folders `ml_dataset_variables` and `ml_dataset_literals` inside each dataset directory.
  It builds the bipartite graphs for variables and literals, associates them with their corresponding backbones, and saves them to the respective folders.

* **`2_create_partitions.py`**
  Creates a `partition.pkl` file inside each dataset directory.
  This file specifies which instances are used for **training**, **validation**, and **testing** in both machine learning and trust-region experiments.

* **`3_train.py`**
  Trains the machine learning model according to the provided training configuration.

* **`4_create_trust_region.py`**
  Generates the trust region used in the optimization process.

  * During **validation**, it proposes new parameter configurations based on the history of previous runs.
  * During **testing**, it uses the best parameters found during validation.

* **`5_run_gurobi.sh`**
  Runs the Gurobi optimizer on a specified folder of instances.

#### Core Modules

* **`constants.py`** – Contains shared constants used throughout the pipeline.
* **`create_trust_region.py`** – Defines the functions for trust region creation.
* **`GCN.py`** – Implements the Graph Convolutional Network architecture.
* **`get_bipartite_graph.py`** – Provides functions for constructing bipartite graphs from datasets.
* **`tpe_optimization.py`** – Implements the Tree-structured Parzen Estimator (TPE) used for parameter optimization in the trust-region generation process.

## Step-by-Step Usage

### 0. Extract the Backbones and Prepare the Dataset

Prepare your dataset by placing all instance files inside:

```
dataset/DATASET_NAME/instance/
```

#### If you **don’t have** the backbones of your instances:

Run the backbone extraction script:

```bash
./0_run_backbone_extraction.sh /dataset/DATASET_NAME/instance '*.opb'
```

You may want to skip backbone extraction for test instances, since their backbones will not be used.

This will generate the following folders:

* `/dataset/DATASET_NAME/backbone/`
  Contains one `INSTANCE_NAME.backbone` file per instance.

* `/dataset/DATASET_NAME/backbone_extraction_log/`
  Contains one `INSTANCE_NAME.log` file per instance, recording extraction details.

#### If you **already have** the backbones:

You can place them manually inside:

```
/dataset/DATASET_NAME/backbone/
```

Make sure that the file names match the instances inside `/dataset/DATASET_NAME/instance/`.
For example, if the instance file is `instance.opb`, the corresponding backbone file should be named `instance.opb.backbone`.

---

### 1. Create the Machine Learning Dataset

This step creates the bipartite graphs and merges the backbone information for each instance into a `.pkl` file used to train the neural network.

Run:

```bash
./1_create_ml_dataset.py --dataset_path /dataset/DATASET_NAME/ --graph_type GRAPH_TYPE
```

Where:

* `GRAPH_TYPE` can be either `literals` or `variables`.

Depending on the type selected, a folder named:

```
dataset/DATASET_NAME/ml_dataset_GRAPH_TYPE/
```

will be created.

---

### 2. Create Experiment Partitions

Before training, you need to create a `partitions.pkl` file.
This file specifies which instances belong to each partition.
Five partitions are expected:

* **ml_train**: Instances used to train the neural network.
* **ml_valid**: Instances used for validation (e.g., computing validation loss, selecting the best epoch, comparing architectures).
* **ml_test**: Instances used to compute test metrics of the neural network (optional for full pipeline execution).
* **trust_region_valid**: Instances used to select the best parameters for trust region construction.
* **trust_region_test**: Instances used to compute the final metrics and evaluate overall performance.

Note: `ml_valid` and `trust_region_valid` may share instances. The same applies to `ml_test` and `trust_region_test`.

Run:

```bash
./2_create_partitions.py --dataset_path /dataset/DATASET_NAME/ --ml_dataset_path /dataset/DATASET_NAME/ml_dataset_GRAPH_TYPE --wkdir_path /wkdir
```

By default, the script assumes instances in `/dataset/DATASET_NAME/instance/` use prefixes indicating their partition (e.g., `train_`, `valid_`, `test_`).
You can modify these defaults using command-line arguments.
For details, run:

```bash
./2_create_partitions.py --help
```

This command will create:

```
/wkdir/DATASET_NAME/
```

containing the files:

* `partitions.pkl` – specifies the partition for each instance.
* `readme.txt` – summarizes the number of instances per partition.

You may also create your own custom partitioning script if more flexibility is needed.

---

### 3. Train the Network

Train the model using:

```bash
./3_train.py --ml_dataset_path /dataset/DATASET_NAME/ml_dataset_GRAPH_TYPE --dataset_wkdir_path /wkdir/DATASET_NAME
```

This script creates the folder:

```
/wkdir/ml_training/ARCHITECTURE_CONFIG/
```

where `ARCHITECTURE_CONFIG` depends on the specific neural network architecture and input type.
By default, it will be named `graph_with_literals_8_GTR`.

To modify the architecture or other training parameters, use the script’s arguments:

```bash
./3_train.py --help
```

Inside `/wkdir/ml_training/ARCHITECTURE_CONFIG/`, the following files are generated:

* `best_model.pth`: Model weights from the best validation epoch.
* `last_model.pth`: Model weights from the final epoch.
* `last_optimizer.pth`: Optimizer state from the last epoch.
* `training_log.csv`: Contains training and validation metrics per epoch.
  If an `ml_test` partition is available, it will also include test metrics.

---

### 4. Create the Trust Region

Once the network is trained, you can create trust regions for new instances.
This process consists of two phases: **Validation** and **Testing**.

---

#### Validation Phase

Used to find the best parameters for trust region construction.

Run:

```bash
./4_create_trust_region.py --dataset_path /dataset/DATASET_NAME --dataset_wkdir_path /wkdir/DATASET_NAME --method METHOD --model_path /wkdir/ml_training/ARCHITECTURE_CONFIG/best_model.pth --run_purpose validation
```

Where `METHOD` can be one of the following:

* `thresholded_expected_error`: Optimizes θ and α parameters.
* `thresholded_weighted_budget`: Optimizes θ and budget parameters.
* `fixed_three_ratios`: Optimizes k₀, k₁, and Δ parameters.
* `fixed_two_ratios`: Optimizes k and Δ parameters.

This creates:

```
/wkdir/DATASET_NAME/validation/ARCHITECTURE_CONFIG/METHOD/
```

Inside this folder:

* Each subfolder corresponds to a different parameter configuration.
* `results.csv` lists all parameter configurations and their `objective_value` column (to be filled after running a solver on each instance).

The best parameter configuration is selected based on the **mean of the objective values** (typically the mean primal integral).

After filling in the `objective_value` column, you can re-run the same script with the same parameters.
It will create new trials that can again be evaluated.
This iterative process can be repeated multiple times — in our experiments, we typically run it **four times** (first creating 30 random trials, then 10 guided trials in each subsequent run).

---

#### Testing Phase

Once the best parameters are selected, run:

```bash
./4_create_trust_region.py --dataset_path /dataset/DATASET_NAME --dataset_wkdir_path /wkdir/DATASET_NAME --method METHOD --model_path /wkdir/ml_training/ARCHITECTURE_CONFIG/best_model.pth --run_purpose test
```

This reads the best configuration from:

```
/wkdir/DATASET_NAME/validation/ARCHITECTURE_CONFIG/METHOD/results.csv
```

and creates:

```
/wkdir/DATASET_NAME/test/ARCHITECTURE_CONFIG/METHOD/BEST_PARAMETERS/
```

Inside, you’ll find the trust regions generated for each instance in the `trust_region_test` partition.

## Acknowledgment

This project builds upon code from the repository [Predict-and-Search_MILP_method](https://github.com/sribdcn/Predict-and-Search_MILP_method), which is licensed under the MIT License. The original license is included in the `third_party/` folder of this repository.

The original code is associated with the following paper, which introduces the Predict-and-Search framework:

> Han, Q., Yang, L., Chen, Q., Zhou, X., Zhang, D., Wang, A., ... & Luo, X. (2023).  
> *A GNN-guided Predict-and-Search Framework for Mixed-Integer Linear Programming.*  
> [arXiv:2302.05636](https://arxiv.org/abs/2302.05636)
